apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: bearing-sensor-data-training-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.0.4, pipelines.kubeflow.org/pipeline_compilation_time: '2020-12-04T17:05:48.022061',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "The pipeline for training
      and deploying an anomaly detector based on an autoencoder", "inputs": [{"name":
      "project_id"}, {"name": "region"}, {"name": "source_bucket_name"}, {"name":
      "prefix"}, {"name": "dest_bucket_name"}, {"name": "dest_file_name"}, {"default":
      "gs://rrusson-kubeflow-test", "name": "gcs_root", "optional": true}, {"default":
      "US", "name": "dataset_location", "optional": true}], "name": "Bearing Sensor
      Data Training"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.0.4}
spec:
  entrypoint: bearing-sensor-data-training
  templates:
  - name: bearing-sensor-data-training
    inputs:
      parameters:
      - {name: dest_bucket_name}
      - {name: dest_file_name}
      - {name: gcs_root}
      - {name: prefix}
      - {name: project_id}
      - {name: region}
      - {name: source_bucket_name}
    dag:
      tasks:
      - name: disp-loss
        template: disp-loss
        dependencies: [submitting-a-cloud-ml-training-job-as-a-pipeline-step]
        arguments:
          parameters:
          - {name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-job_id, value: '{{tasks.submitting-a-cloud-ml-training-job-as-a-pipeline-step.outputs.parameters.submitting-a-cloud-ml-training-job-as-a-pipeline-step-job_id}}'}
      - name: disp-loss-2
        template: disp-loss-2
        dependencies: [submitting-a-cloud-ml-training-job-as-a-pipeline-step]
        arguments:
          parameters:
          - {name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-job_id, value: '{{tasks.submitting-a-cloud-ml-training-job-as-a-pipeline-step.outputs.parameters.submitting-a-cloud-ml-training-job-as-a-pipeline-step-job_id}}'}
      - name: load-raw-data
        template: load-raw-data
        arguments:
          parameters:
          - {name: dest_bucket_name, value: '{{inputs.parameters.dest_bucket_name}}'}
          - {name: dest_file_name, value: '{{inputs.parameters.dest_file_name}}'}
          - {name: prefix, value: '{{inputs.parameters.prefix}}'}
          - {name: source_bucket_name, value: '{{inputs.parameters.source_bucket_name}}'}
      - name: run-data-decsribe
        template: run-data-decsribe
        dependencies: [load-raw-data]
        arguments:
          parameters:
          - {name: gcs_root, value: '{{inputs.parameters.gcs_root}}'}
          - {name: load-raw-data-dest_file_name, value: '{{tasks.load-raw-data.outputs.parameters.load-raw-data-dest_file_name}}'}
      - name: split-data
        template: split-data
        dependencies: [load-raw-data]
        arguments:
          parameters:
          - {name: load-raw-data-dest_bucket_name, value: '{{tasks.load-raw-data.outputs.parameters.load-raw-data-dest_bucket_name}}'}
          - {name: load-raw-data-dest_file_name, value: '{{tasks.load-raw-data.outputs.parameters.load-raw-data-dest_file_name}}'}
      - name: submitting-a-cloud-ml-training-job-as-a-pipeline-step
        template: submitting-a-cloud-ml-training-job-as-a-pipeline-step
        dependencies: [split-data]
        arguments:
          parameters:
          - {name: gcs_root, value: '{{inputs.parameters.gcs_root}}'}
          - {name: project_id, value: '{{inputs.parameters.project_id}}'}
          - {name: region, value: '{{inputs.parameters.region}}'}
          - {name: split-data-bucket_name, value: '{{tasks.split-data.outputs.parameters.split-data-bucket_name}}'}
          - {name: split-data-test_dest_file, value: '{{tasks.split-data.outputs.parameters.split-data-test_dest_file}}'}
          - {name: split-data-train_dest_file, value: '{{tasks.split-data.outputs.parameters.split-data-train_dest_file}}'}
  - name: disp-loss
    container:
      args: [--job-id, '{{inputs.parameters.submitting-a-cloud-ml-training-job-as-a-pipeline-step-job_id}}',
        '----output-paths', /tmp/outputs/Output/data]
      command:
      - python3
      - -u
      - -c
      - "def disp_loss(job_id):\n\n    import json\n\n    metadata = {\n        'outputs'\
        \ : [{\n        'type': 'web-app',\n        'storage': 'inline',\n       \
        \ 'source': '<h1>Hello, World!</h1>',\n        }]\n    }\n\n    with open('/mlpipeline-ui-metadata.json',\
        \ 'w') as f: \n        json_string = json.dumps(metadata)\n        f.write(json_string)\
        \ \n\n    return job_id\n\ndef _serialize_str(str_value: str) -> str:\n  \
        \  if not isinstance(str_value, str):\n        raise TypeError('Value \"{}\"\
        \ has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n\
        \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Disp\
        \ loss', description='')\n_parser.add_argument(\"--job-id\", dest=\"job_id\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        ----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args\
        \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
        , [])\n\n_outputs = disp_loss(**_parsed_args)\n\n_outputs = [_outputs]\n\n\
        _output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file\
        \ in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: python:3.7
    inputs:
      parameters:
      - {name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-job_id}
    outputs:
      artifacts:
      - {name: disp-loss-Output, path: /tmp/outputs/Output/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--job-id", {"inputValue": "job_id"}, "----output-paths", {"outputPath":
          "Output"}], "command": ["python3", "-u", "-c", "def disp_loss(job_id):\n\n    import
          json\n\n    metadata = {\n        ''outputs'' : [{\n        ''type'': ''web-app'',\n        ''storage'':
          ''inline'',\n        ''source'': ''<h1>Hello, World!</h1>'',\n        }]\n    }\n\n    with
          open(''/mlpipeline-ui-metadata.json'', ''w'') as f: \n        json_string
          = json.dumps(metadata)\n        f.write(json_string) \n\n    return job_id\n\ndef
          _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,
          str):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          str.''.format(str(str_value), str(type(str_value))))\n    return str_value\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Disp loss'', description='''')\n_parser.add_argument(\"--job-id\",
          dest=\"job_id\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = disp_loss(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "python:3.7"}}, "inputs": [{"name": "job_id", "type": "String"}],
          "name": "Disp loss", "outputs": [{"name": "Output", "type": "String"}]}',
        pipelines.kubeflow.org/component_ref: '{}'}
  - name: disp-loss-2
    container:
      args: [--job-id, '{{inputs.parameters.submitting-a-cloud-ml-training-job-as-a-pipeline-step-job_id}}',
        '----output-paths', /tmp/outputs/Output/data]
      command:
      - python3
      - -u
      - -c
      - "def disp_loss(job_id):\n\n    import json\n\n    metadata = {\n        'outputs'\
        \ : [{\n        'type': 'web-app',\n        'storage': 'inline',\n       \
        \ 'source': '<h1>Hello, World!</h1>',\n        }]\n    }\n\n    with open('/mlpipeline-ui-metadata.json',\
        \ 'w') as f: \n        json_string = json.dumps(metadata)\n        f.write(json_string)\
        \ \n\n    return job_id\n\ndef _serialize_str(str_value: str) -> str:\n  \
        \  if not isinstance(str_value, str):\n        raise TypeError('Value \"{}\"\
        \ has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n\
        \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Disp\
        \ loss', description='')\n_parser.add_argument(\"--job-id\", dest=\"job_id\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        ----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args\
        \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
        , [])\n\n_outputs = disp_loss(**_parsed_args)\n\n_outputs = [_outputs]\n\n\
        _output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file\
        \ in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: python:3.7
    inputs:
      parameters:
      - {name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-job_id}
    outputs:
      artifacts:
      - {name: disp-loss-2-Output, path: /tmp/outputs/Output/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--job-id", {"inputValue": "job_id"}, "----output-paths", {"outputPath":
          "Output"}], "command": ["python3", "-u", "-c", "def disp_loss(job_id):\n\n    import
          json\n\n    metadata = {\n        ''outputs'' : [{\n        ''type'': ''web-app'',\n        ''storage'':
          ''inline'',\n        ''source'': ''<h1>Hello, World!</h1>'',\n        }]\n    }\n\n    with
          open(''/mlpipeline-ui-metadata.json'', ''w'') as f: \n        json_string
          = json.dumps(metadata)\n        f.write(json_string) \n\n    return job_id\n\ndef
          _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,
          str):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          str.''.format(str(str_value), str(type(str_value))))\n    return str_value\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Disp loss'', description='''')\n_parser.add_argument(\"--job-id\",
          dest=\"job_id\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = disp_loss(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "python:3.7"}}, "inputs": [{"name": "job_id", "type": "String"}],
          "name": "Disp loss", "outputs": [{"name": "Output", "type": "String"}]}',
        pipelines.kubeflow.org/component_ref: '{}'}
  - name: load-raw-data
    container:
      args: [--source-bucket-name, '{{inputs.parameters.source_bucket_name}}', --prefix,
        '{{inputs.parameters.prefix}}', --dest-bucket-name, '{{inputs.parameters.dest_bucket_name}}',
        --dest-file-name, '{{inputs.parameters.dest_file_name}}', '----output-paths',
        /tmp/outputs/dest_bucket_name/data, /tmp/outputs/dest_file_name/data]
      command:
      - python3
      - -u
      - -c
      - "def load_raw_data(source_bucket_name, \n                  prefix,\n     \
        \             dest_bucket_name,\n                  dest_file_name):\n\n  \
        \  \"\"\"Retrieves the sample files, combines them, and outputs the desting\
        \ location in GCS.\"\"\"\n    import pandas as pd\n    import numpy as np\n\
        \    from io import StringIO\n    from google.cloud import storage\n\n   \
        \ # Get the raw files out of GCS public bucket\n    merged_data = pd.DataFrame()\n\
        \    client = storage.Client()\n    blobs = client.list_blobs(source_bucket_name,\
        \ prefix=prefix)\n\n    for blob in blobs:\n        dataset = pd.read_csv(\"\
        gs://{0}/{1}\".format(source_bucket_name, blob.name), sep='\\t')\n       \
        \ dataset_mean_abs = np.array(dataset.abs().mean())\n        dataset_mean_abs\
        \ = pd.DataFrame(dataset_mean_abs.reshape(1, 4))\n        dataset_mean_abs.index\
        \ = [blob.name.split(\"/\")[-1]]\n        merged_data = merged_data.append(dataset_mean_abs)\n\
        \n    merged_data.columns = ['bearing-1', 'bearing-2', 'bearing-3', 'bearing-4']\n\
        \n    # Transform data file index to datetime and sort in chronological order\n\
        \    merged_data.index = pd.to_datetime(merged_data.index, format='%Y.%m.%d.%H.%M.%S')\n\
        \    merged_data = merged_data.sort_index()\n\n    # Drop the raw_data into\
        \ a bucket\n    #DEST_FILE_NAME = \"raw_data.csv\"\n    #DEST_BUCKET_NAME\
        \ = \"rrusson-kubeflow-test\"\n    f = StringIO()\n    merged_data.to_csv(f)\n\
        \    f.seek(0)\n    client.get_bucket(dest_bucket_name).blob(dest_file_name).upload_from_file(f,\
        \ content_type='text/csv')\n\n    return (dest_bucket_name, dest_file_name)\n\
        \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
        \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of\
        \ str.'.format(str(str_value), str(type(str_value))))\n    return str_value\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Load raw data',\
        \ description='Retrieves the sample files, combines them, and outputs the\
        \ desting location in GCS.')\n_parser.add_argument(\"--source-bucket-name\"\
        , dest=\"source_bucket_name\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--prefix\", dest=\"prefix\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dest-bucket-name\"\
        , dest=\"dest_bucket_name\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dest-file-name\", dest=\"dest_file_name\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
        , dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = load_raw_data(**_parsed_args)\n\
        \n_output_serializers = [\n    _serialize_str,\n    _serialize_str,\n\n]\n\
        \nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n\
        \        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: gcr.io/mwpmltr/rrusson_kubeflow_base:v1
    inputs:
      parameters:
      - {name: dest_bucket_name}
      - {name: dest_file_name}
      - {name: prefix}
      - {name: source_bucket_name}
    outputs:
      parameters:
      - name: load-raw-data-dest_bucket_name
        valueFrom: {path: /tmp/outputs/dest_bucket_name/data}
      - name: load-raw-data-dest_file_name
        valueFrom: {path: /tmp/outputs/dest_file_name/data}
      artifacts:
      - {name: load-raw-data-dest_bucket_name, path: /tmp/outputs/dest_bucket_name/data}
      - {name: load-raw-data-dest_file_name, path: /tmp/outputs/dest_file_name/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Retrieves
          the sample files, combines them, and outputs the desting location in GCS.",
          "implementation": {"container": {"args": ["--source-bucket-name", {"inputValue":
          "source_bucket_name"}, "--prefix", {"inputValue": "prefix"}, "--dest-bucket-name",
          {"inputValue": "dest_bucket_name"}, "--dest-file-name", {"inputValue": "dest_file_name"},
          "----output-paths", {"outputPath": "dest_bucket_name"}, {"outputPath": "dest_file_name"}],
          "command": ["python3", "-u", "-c", "def load_raw_data(source_bucket_name,
          \n                  prefix,\n                  dest_bucket_name,\n                  dest_file_name):\n\n    \"\"\"Retrieves
          the sample files, combines them, and outputs the desting location in GCS.\"\"\"\n    import
          pandas as pd\n    import numpy as np\n    from io import StringIO\n    from
          google.cloud import storage\n\n    # Get the raw files out of GCS public
          bucket\n    merged_data = pd.DataFrame()\n    client = storage.Client()\n    blobs
          = client.list_blobs(source_bucket_name, prefix=prefix)\n\n    for blob in
          blobs:\n        dataset = pd.read_csv(\"gs://{0}/{1}\".format(source_bucket_name,
          blob.name), sep=''\\t'')\n        dataset_mean_abs = np.array(dataset.abs().mean())\n        dataset_mean_abs
          = pd.DataFrame(dataset_mean_abs.reshape(1, 4))\n        dataset_mean_abs.index
          = [blob.name.split(\"/\")[-1]]\n        merged_data = merged_data.append(dataset_mean_abs)\n\n    merged_data.columns
          = [''bearing-1'', ''bearing-2'', ''bearing-3'', ''bearing-4'']\n\n    #
          Transform data file index to datetime and sort in chronological order\n    merged_data.index
          = pd.to_datetime(merged_data.index, format=''%Y.%m.%d.%H.%M.%S'')\n    merged_data
          = merged_data.sort_index()\n\n    # Drop the raw_data into a bucket\n    #DEST_FILE_NAME
          = \"raw_data.csv\"\n    #DEST_BUCKET_NAME = \"rrusson-kubeflow-test\"\n    f
          = StringIO()\n    merged_data.to_csv(f)\n    f.seek(0)\n    client.get_bucket(dest_bucket_name).blob(dest_file_name).upload_from_file(f,
          content_type=''text/csv'')\n\n    return (dest_bucket_name, dest_file_name)\n\ndef
          _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,
          str):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          str.''.format(str(str_value), str(type(str_value))))\n    return str_value\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Load raw data'', description=''Retrieves
          the sample files, combines them, and outputs the desting location in GCS.'')\n_parser.add_argument(\"--source-bucket-name\",
          dest=\"source_bucket_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--prefix\",
          dest=\"prefix\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dest-bucket-name\",
          dest=\"dest_bucket_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dest-file-name\",
          dest=\"dest_file_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = load_raw_data(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,
          output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "gcr.io/mwpmltr/rrusson_kubeflow_base:v1"}}, "inputs": [{"name":
          "source_bucket_name", "type": "String"}, {"name": "prefix", "type": "String"},
          {"name": "dest_bucket_name", "type": "String"}, {"name": "dest_file_name",
          "type": "String"}], "name": "Load raw data", "outputs": [{"name": "dest_bucket_name",
          "type": "String"}, {"name": "dest_file_name", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: run-data-decsribe
    container:
      args: [--gcs_root, '{{inputs.parameters.gcs_root}}', --file, '{{inputs.parameters.load-raw-data-dest_file_name}}']
      image: gcr.io/mwpmltr/rrusson_kubeflow_datadescribe:v1
    inputs:
      parameters:
      - {name: gcs_root}
      - {name: load-raw-data-dest_file_name}
  - name: split-data
    container:
      args:
      - --bucket-name
      - '{{inputs.parameters.load-raw-data-dest_bucket_name}}'
      - --source-file
      - '{{inputs.parameters.load-raw-data-dest_file_name}}'
      - --split-time
      - '2004-02-15 12:52:39'
      - --preprocess
      - "True"
      - '----output-paths'
      - /tmp/outputs/bucket_name/data
      - /tmp/outputs/train_dest_file/data
      - /tmp/outputs/test_dest_file/data
      command:
      - python3
      - -u
      - -c
      - "def split_data(bucket_name, \n               source_file,\n             \
        \  split_time, \n               preprocess):\n\n    from sklearn.preprocessing\
        \ import MinMaxScaler\n    from google.cloud import storage\n    import pandas\
        \ as pd\n    import numpy as np\n    from io import StringIO\n    import time\n\
        \n    # Read in the data from the GCS bucket and format the data\n    data_loc\
        \ = \"gs://{0}/{1}\".format(bucket_name, source_file)\n    data = pd.read_csv(data_loc,\
        \ index_col=0)\n    #data.index.rename('time', inplace=True)\n    first_idx\
        \ = data.index.values[0]\n\n    # Split the data based on the split_time param\n\
        \    data = data.sort_index()\n    train_data = data.loc[first_idx:split_time]\
        \  # Note: this is 'inclusive' so the last data point in train data\n    test_data\
        \ = data.loc[split_time:]            # shows up as the first data point in\
        \ the test data\n                                                 # This shouldn't\
        \ be a big deal for this dataset\n\n    # Preprocess the data (if applicable)\n\
        \    if preprocess:\n        scaler = MinMaxScaler()\n        X_train = scaler.fit_transform(train_data)\n\
        \        X_test = scaler.transform(test_data)\n\n    else:\n        X_train\
        \ = train_data.to_numpy()\n        X_test = test_data.to_numpy()\n\n    scaled_train_data\
        \ = pd.DataFrame(X_train, columns=data.columns)\n    scaled_test_data = pd.DataFrame(X_test,\
        \ columns=data.columns)\n\n    # Save the data splits off to GCS bucket\n\
        \    train_f = StringIO()\n    test_f = StringIO()\n\n    scaled_train_data.to_csv(train_f)\n\
        \    scaled_test_data.to_csv(test_f)\n\n    train_f.seek(0)\n    test_f.seek(0)\n\
        \n    train_dest_file = \"train_{}.csv\".format(time.perf_counter())\n   \
        \ test_dest_file = \"test_{}.csv\".format(time.perf_counter())\n\n    client\
        \ = storage.Client()\n    client.get_bucket(bucket_name).blob(train_dest_file).upload_from_file(train_f,\
        \ content_type='text/csv')\n    client.get_bucket(bucket_name).blob(test_dest_file).upload_from_file(test_f,\
        \ content_type='text/csv')\n\n    # Return the location of the new data splits\n\
        \    return (bucket_name, train_dest_file, test_dest_file)\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
        \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of\
        \ str.'.format(str(str_value), str(type(str_value))))\n    return str_value\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Split data', description='')\n\
        _parser.add_argument(\"--bucket-name\", dest=\"bucket_name\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--source-file\", dest=\"\
        source_file\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --split-time\", dest=\"split_time\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--preprocess\", dest=\"preprocess\", type=_deserialize_bool,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
        , dest=\"_output_paths\", type=str, nargs=3)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = split_data(**_parsed_args)\n\
        \n_output_serializers = [\n    _serialize_str,\n    _serialize_str,\n    _serialize_str,\n\
        \n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n\
        \        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: gcr.io/mwpmltr/rrusson_kubeflow_base:v1
    inputs:
      parameters:
      - {name: load-raw-data-dest_bucket_name}
      - {name: load-raw-data-dest_file_name}
    outputs:
      parameters:
      - name: split-data-bucket_name
        valueFrom: {path: /tmp/outputs/bucket_name/data}
      - name: split-data-test_dest_file
        valueFrom: {path: /tmp/outputs/test_dest_file/data}
      - name: split-data-train_dest_file
        valueFrom: {path: /tmp/outputs/train_dest_file/data}
      artifacts:
      - {name: split-data-bucket_name, path: /tmp/outputs/bucket_name/data}
      - {name: split-data-test_dest_file, path: /tmp/outputs/test_dest_file/data}
      - {name: split-data-train_dest_file, path: /tmp/outputs/train_dest_file/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--bucket-name", {"inputValue": "bucket_name"}, "--source-file",
          {"inputValue": "source_file"}, "--split-time", {"inputValue": "split_time"},
          "--preprocess", {"inputValue": "preprocess"}, "----output-paths", {"outputPath":
          "bucket_name"}, {"outputPath": "train_dest_file"}, {"outputPath": "test_dest_file"}],
          "command": ["python3", "-u", "-c", "def split_data(bucket_name, \n               source_file,\n               split_time,
          \n               preprocess):\n\n    from sklearn.preprocessing import MinMaxScaler\n    from
          google.cloud import storage\n    import pandas as pd\n    import numpy as
          np\n    from io import StringIO\n    import time\n\n    # Read in the data
          from the GCS bucket and format the data\n    data_loc = \"gs://{0}/{1}\".format(bucket_name,
          source_file)\n    data = pd.read_csv(data_loc, index_col=0)\n    #data.index.rename(''time'',
          inplace=True)\n    first_idx = data.index.values[0]\n\n    # Split the data
          based on the split_time param\n    data = data.sort_index()\n    train_data
          = data.loc[first_idx:split_time]  # Note: this is ''inclusive'' so the last
          data point in train data\n    test_data = data.loc[split_time:]            #
          shows up as the first data point in the test data\n                                                 #
          This shouldn''t be a big deal for this dataset\n\n    # Preprocess the data
          (if applicable)\n    if preprocess:\n        scaler = MinMaxScaler()\n        X_train
          = scaler.fit_transform(train_data)\n        X_test = scaler.transform(test_data)\n\n    else:\n        X_train
          = train_data.to_numpy()\n        X_test = test_data.to_numpy()\n\n    scaled_train_data
          = pd.DataFrame(X_train, columns=data.columns)\n    scaled_test_data = pd.DataFrame(X_test,
          columns=data.columns)\n\n    # Save the data splits off to GCS bucket\n    train_f
          = StringIO()\n    test_f = StringIO()\n\n    scaled_train_data.to_csv(train_f)\n    scaled_test_data.to_csv(test_f)\n\n    train_f.seek(0)\n    test_f.seek(0)\n\n    train_dest_file
          = \"train_{}.csv\".format(time.perf_counter())\n    test_dest_file = \"test_{}.csv\".format(time.perf_counter())\n\n    client
          = storage.Client()\n    client.get_bucket(bucket_name).blob(train_dest_file).upload_from_file(train_f,
          content_type=''text/csv'')\n    client.get_bucket(bucket_name).blob(test_dest_file).upload_from_file(test_f,
          content_type=''text/csv'')\n\n    # Return the location of the new data
          splits\n    return (bucket_name, train_dest_file, test_dest_file)\n\ndef
          _deserialize_bool(s) -> bool:\n    from distutils.util import strtobool\n    return
          strtobool(s) == 1\n\ndef _serialize_str(str_value: str) -> str:\n    if
          not isinstance(str_value, str):\n        raise TypeError(''Value \"{}\"
          has type \"{}\" instead of str.''.format(str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Split
          data'', description='''')\n_parser.add_argument(\"--bucket-name\", dest=\"bucket_name\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--source-file\",
          dest=\"source_file\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--split-time\",
          dest=\"split_time\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--preprocess\",
          dest=\"preprocess\", type=_deserialize_bool, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = split_data(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n    _serialize_str,\n    _serialize_str,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "gcr.io/mwpmltr/rrusson_kubeflow_base:v1"}}, "inputs": [{"name":
          "bucket_name", "type": "String"}, {"name": "source_file", "type": "String"},
          {"name": "split_time", "type": "String"}, {"name": "preprocess", "type":
          "Boolean"}], "name": "Split data", "outputs": [{"name": "bucket_name", "type":
          "String"}, {"name": "train_dest_file", "type": "String"}, {"name": "test_dest_file",
          "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: submitting-a-cloud-ml-training-job-as-a-pipeline-step
    container:
      args: [--ui_metadata_path, /tmp/outputs/MLPipeline_UI_metadata/data, kfp_component.google.ml_engine,
        train, --project_id, '{{inputs.parameters.project_id}}', --python_module,
        '', --package_uris, '', --region, '{{inputs.parameters.region}}', --args,
        '["--bucket", "{{inputs.parameters.split-data-bucket_name}}", "--train_file",
          "{{inputs.parameters.split-data-train_dest_file}}", "--test_file", "{{inputs.parameters.split-data-test_dest_file}}"]',
        --job_dir, '{{inputs.parameters.gcs_root}}/jobdir/{{workflow.uid}}', --python_version,
        '', --runtime_version, '', --master_image_uri, 'gcr.io/mwpmltr/rrusson_kubeflow_tf2_trainer:v5',
        --worker_image_uri, '', --training_input, '', --job_id_prefix, anomaly-detection_,
        --wait_interval, '30']
      command: []
      env:
      - {name: KFP_POD_NAME, value: '{{pod.name}}'}
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      image: gcr.io/ml-pipeline/ml-pipeline-gcp:e66dcb18607406330f953bf99b04fe7c3ed1a4a8
    inputs:
      parameters:
      - {name: gcs_root}
      - {name: project_id}
      - {name: region}
      - {name: split-data-bucket_name}
      - {name: split-data-test_dest_file}
      - {name: split-data-train_dest_file}
    outputs:
      parameters:
      - name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-job_id
        valueFrom: {path: /tmp/kfp/output/ml_engine/job_id.txt}
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/MLPipeline_UI_metadata/data}
      - {name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-job_dir, path: /tmp/kfp/output/ml_engine/job_dir.txt}
      - {name: submitting-a-cloud-ml-training-job-as-a-pipeline-step-job_id, path: /tmp/kfp/output/ml_engine/job_id.txt}
    metadata:
      labels:
        add-pod-env: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "A Kubeflow
          Pipeline component to submit a Cloud Machine Learning (Cloud ML) \nEngine
          training job as a step in a pipeline.\n", "implementation": {"container":
          {"args": ["--ui_metadata_path", {"outputPath": "MLPipeline UI metadata"},
          "kfp_component.google.ml_engine", "train", "--project_id", {"inputValue":
          "project_id"}, "--python_module", {"inputValue": "python_module"}, "--package_uris",
          {"inputValue": "package_uris"}, "--region", {"inputValue": "region"}, "--args",
          {"inputValue": "args"}, "--job_dir", {"inputValue": "job_dir"}, "--python_version",
          {"inputValue": "python_version"}, "--runtime_version", {"inputValue": "runtime_version"},
          "--master_image_uri", {"inputValue": "master_image_uri"}, "--worker_image_uri",
          {"inputValue": "worker_image_uri"}, "--training_input", {"inputValue": "training_input"},
          "--job_id_prefix", {"inputValue": "job_id_prefix"}, "--wait_interval", {"inputValue":
          "wait_interval"}], "env": {"KFP_POD_NAME": "{{pod.name}}"}, "fileOutputs":
          {"job_dir": "/tmp/kfp/output/ml_engine/job_dir.txt", "job_id": "/tmp/kfp/output/ml_engine/job_id.txt"},
          "image": "gcr.io/ml-pipeline/ml-pipeline-gcp:e66dcb18607406330f953bf99b04fe7c3ed1a4a8"}},
          "inputs": [{"description": "Required. The ID of the parent project of the
          job.", "name": "project_id", "type": "GCPProjectID"}, {"default": "", "description":
          "The Python module name to run after installing the packages.", "name":
          "python_module", "type": "String"}, {"default": "", "description": "The
          Cloud Storage location of the packages (that contain the training program  and
          any additional dependencies). The maximum number of package URIs is 100.",
          "name": "package_uris", "type": "List"}, {"default": "", "description":
          "The Compute Engine region in which the training job is run.", "name": "region",
          "type": "GCPRegion"}, {"default": "", "description": "The command line arguments
          to pass to the program.", "name": "args", "type": "List"}, {"default": "",
          "description": "A Cloud Storage path in which to store the training outputs
          and other data  needed for training. This path is passed to your TensorFlow
          program as the  `job-dir` command-line argument. The benefit of specifying
          this field is  that Cloud ML validates the path for use in training.", "name":
          "job_dir", "type": "GCSPath"}, {"default": "", "description": "The version
          of Python used in training. If not set, the default version is `2.7`. Python
          `3.5` is available when runtimeVersion is set to `1.4` and above.", "name":
          "python_version", "type": "String"}, {"default": "", "description": "The
          Cloud ML Engine runtime version to use for training. If not set, Cloud ML
          Engine uses the default stable version, 1.0.", "name": "runtime_version",
          "type": "String"}, {"default": "", "description": "The Docker image to run
          on the master replica. This image must be in Container Registry.", "name":
          "master_image_uri", "type": "GCRPath"}, {"default": "", "description": "The
          Docker image to run on the worker replica. This image must be in Container
          Registry.", "name": "worker_image_uri", "type": "GCRPath"}, {"default":
          "", "description": "The input parameters to create a training job. It is
          the JSON payload  of a [TrainingInput](https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#TrainingInput)",
          "name": "training_input", "type": "Dict"}, {"default": "", "description":
          "The prefix of the generated job id.", "name": "job_id_prefix", "type":
          "String"}, {"default": "30", "description": "Optional. A time-interval to
          wait for between calls to get the job status.  Defaults to 30.''", "name":
          "wait_interval", "type": "Integer"}], "metadata": {"labels": {"add-pod-env":
          "true"}}, "name": "Submitting a Cloud ML training job as a pipeline step",
          "outputs": [{"description": "The ID of the created job.", "name": "job_id",
          "type": "String"}, {"description": "The output path in Cloud Storage of
          the trainning job, which contains  the trained model files.", "name": "job_dir",
          "type": "GCSPath"}, {"name": "MLPipeline UI metadata", "type": "UI metadata"}]}',
        pipelines.kubeflow.org/component_ref: '{"digest": "c05c450e73cf6fc6fd702d86fb6ae06734a7a69f6281d5175e842be39394e206",
          "name": "ml_engine/train", "url": "https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/ml_engine/train/component.yaml"}'}
  arguments:
    parameters:
    - {name: project_id}
    - {name: region}
    - {name: source_bucket_name}
    - {name: prefix}
    - {name: dest_bucket_name}
    - {name: dest_file_name}
    - {name: gcs_root, value: 'gs://rrusson-kubeflow-test'}
    - {name: dataset_location, value: US}
  serviceAccountName: pipeline-runner
